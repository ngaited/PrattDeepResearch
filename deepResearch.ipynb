{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q exa-py langchain langchain-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('/content/drive/MyDrive/env.txt')\n",
    "\n",
    "import os, re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from exa_py import Exa\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exa = Exa(api_key=os.environ[\"EXA_API_KEY\"])\n",
    "\n",
    "@tool\n",
    "def search_and_contents(query: str):\n",
    "    \"\"\"Search for webpages based on the query and retrieve their contents.\"\"\"\n",
    "    # This combines two API endpoints: search and contents retrieval\n",
    "    return exa.search_and_contents(\n",
    "        query, use_autoprompt=True, num_results=5, text=True, highlights=True\n",
    "    )\n",
    "\n",
    "@tool\n",
    "def find_similar_and_contents(url: str):\n",
    "    \"\"\"Search for webpages similar to a given URL and retrieve their contents.\n",
    "    The url passed in should be a URL returned from `search_and_contents`.\n",
    "    \"\"\"\n",
    "    # This combines two API endpoints: find similar and contents retrieval\n",
    "    return exa.find_similar_and_contents(url, num_results=5, text=True, highlights=True)\n",
    "\n",
    "# Initialize the LLMs\n",
    "# Use a more capable model for evaluation/refinement and writing\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "planner_llm = ChatOpenAI(model=\"o3-mini\")\n",
    "\n",
    "# --- 2. Define Tools ---\n",
    "# search_tool = TavilySearchResults(max_results=5)\n",
    "researcher_tools = [search_and_contents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Create Planning Agent ---\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert research planner. Create a detailed, step-by-step research plan for the given topic, suitable for an academic paper. \"\n",
    "            \"Output the plan as a numbered list of specific, concise research questions or objectives.\"\n",
    "            \"Example:\\nTopic: Impact of AI on Healthcare\\nPlan:\\n1. Current AI applications in medical diagnosis?\\n2. AI's role in drug discovery?\\n3. Ethical considerations of AI in healthcare?\\n4. Future trends of AI in healthcare?\\n5. AI's impact on patient outcomes and costs.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Create a research plan for the topic: {topic}\\n\\nResearch Plan:\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Use the designated planner LLM\n",
    "planner_chain = planner_prompt | planner_llm | StrOutputParser()\n",
    "\n",
    "# --- 4. Create Researcher Agent ---\n",
    "# No changes needed here, it takes a specific question\n",
    "researcher_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a diligent research assistant. Your goal is to use the search tool to find comprehensive information on the *specific research question* provided. \"\n",
    "            \"Focus on factual information, key arguments, and evidence relevant *only* to this question.\"\n",
    "            \"Synthesize the findings concisely.\"\n",
    "            \"IMPORTANT: After synthesizing the findings, list the URLs of the primary sources you consulted to gather this information. Each URL should be on a new line, prefixed with 'Source URL: '. \"\n",
    "            \"Example:\\n\\nSynthesized findings go here...\\n\\nSource URL: https://example.com/article1\\nSource URL: https://anothersource.org/data\\n\"\n",
    "            \"Do not add introductions like 'Here's what I found...'. Just provide the synthesis and the source list.\"\n",
    "        ),\n",
    "        (\"human\", \"Research Question: {research_question}\\nProvide detailed search results and list source URLs.\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "# Use the main LLM for research synthesis\n",
    "researcher_agent_runnable = create_openai_functions_agent(llm, researcher_tools, researcher_prompt)\n",
    "researcher_agent_executor = AgentExecutor(agent=researcher_agent_runnable, tools=researcher_tools, verbose=True)\n",
    "\n",
    "# --- 5. Create Evaluator & Refiner Agent ---\n",
    "# This agent now ALSO generates refinement questions based on weaknesses.\n",
    "evaluator_refiner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert academic evaluator. Your tasks are:\\n\"\n",
    "            \"1. Critically assess the provided aggregated research findings based on quality, relevance, depth, and coherence for the original topic. Focus on the synthesized text content, ignoring 'Source URL:' lines for evaluation.\\n\"\n",
    "            \"2. Identify overall strengths, weaknesses, sufficiency, and specific gaps.\\n\"\n",
    "            \"3. **If significant gaps or weaknesses are identified** that require more focused research, formulate 1-3 specific, new research questions to address *only* these weaknesses. These questions should guide a targeted follow-up search.\\n\"\n",
    "            \"4. Structure your output clearly:\\n\"\n",
    "            \"   - Start with 'Evaluation:' followed by your assessment (strengths, weaknesses, gaps).\\n\"\n",
    "            \"   - Then, add a section 'Further Research Needed:'. List the 1-3 new research questions here (numbered), or state 'None' if the initial research is sufficient.\\n\\n\"\n",
    "            \"Example Output (if refinement needed):\\n\"\n",
    "            \"Evaluation:\\n[Your detailed evaluation text here, discussing strengths, weaknesses like lack of detail on topic X, insufficient evidence for claim Y]...\\n\\n\"\n",
    "            \"Further Research Needed:\\n1. What specific quantitative data exists on the impact of X on Y?\\n2. Are there recent case studies challenging the findings on Z?\\n\\n\"\n",
    "            \"Example Output (if sufficient):\\n\"\n",
    "            \"Evaluation:\\n[Your detailed evaluation text, concluding the research is comprehensive and addresses the core areas adequately]...\\n\\n\"\n",
    "            \"Further Research Needed:\\nNone\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Original Research Topic: {topic}\\n\\n\"\n",
    "            \"Aggregated Research Findings:\\n```\\n{research_results}\\n```\\n\\n\"\n",
    "            \"Your Evaluation and Refinement Plan:\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "# Use the main capable LLM for evaluation and refinement planning\n",
    "evaluator_refiner_chain = evaluator_refiner_prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- 6. Create Writer Agent ---\n",
    "# This agent now receives potentially expanded research content and the initial evaluation text.\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an academic writer. Synthesize the **final, potentially expanded, research findings** into a coherent draft section for an academic paper (e.g., literature review, key findings) based on the original topic and the provided evaluation.\"\n",
    "            \"The research findings may include initial results and potentially follow-up research addressing identified weaknesses.\\n\"\n",
    "            \"Use a formal, objective tone and structure the information logically in Markdown format.\\n\"\n",
    "            \"You have been provided with:\\n\"\n",
    "            \"   - An evaluation of the *initial* research.\\n\"\n",
    "            \"   - The *combined* research findings (initial + any refinement).\\n\"\n",
    "            \"   - A list of all source URLs consulted across all research stages.\\n\"\n",
    "            \"Your primary task is to synthesize the **combined research findings**, ensuring the final text addresses points raised in the evaluation where possible.\\n\"\n",
    "            \"Attempt to incorporate references where appropriate (e.g., '[citation needed]' or '(Source: [URL snippet])').\\n\"\n",
    "            \"Conclude the report with a 'References' section listing ALL unique source URLs provided.\\n\"\n",
    "            \"Format the 'References' section clearly (e.g., a bulleted list).\\n\"\n",
    "            \"Start the output directly with the report section.\"\n",
    "        ),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Original Research Topic: {topic}\\n\\n\"\n",
    "            \"Evaluation of Initial Research:\\n```\\n{evaluation_text}\\n```\\n\\n\"\n",
    "            \"Combined Research Findings to Synthesize:\\n```\\n{combined_research_findings}\\n```\\n\\n\"\n",
    "            \"List of All Source URLs Consulted:\\n{source_urls}\\n\\n\"\n",
    "            \"Draft Academic Paper Section (in Markdown, including References section):\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "writer_chain = writer_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "# --- Helper Function to Parse Plan ---\n",
    "def parse_research_plan(plan_string: str) -> list[str]:\n",
    "    \"\"\"Parses the numbered/bulleted list plan output into a list of strings.\"\"\"\n",
    "    plan_items = []\n",
    "    # Try numbered list first\n",
    "    matches = re.findall(r\"^\\s*\\d+\\.\\s+(.*)\", plan_string, re.MULTILINE)\n",
    "    if not matches:\n",
    "        # Fallback: Try bullet points or dashes\n",
    "        matches = re.findall(r\"^\\s*[\\*\\-]\\s+(.*)\", plan_string, re.MULTILINE)\n",
    "\n",
    "    if matches:\n",
    "         for item in matches:\n",
    "            plan_items.append(item.strip())\n",
    "    elif plan_string.strip(): # If no list format detected, but there's content\n",
    "        # Fallback: Split by newline, basic filtering\n",
    "        lines = plan_string.strip().split('\\n')\n",
    "        plan_items = [line.strip() for line in lines if line.strip() and len(line.strip()) > 5] # Basic check\n",
    "\n",
    "    # If still no items, but input wasn't empty, treat as one step\n",
    "    if not plan_items and plan_string.strip():\n",
    "        print(\"Warning: Could not parse plan into distinct steps. Treating the whole output as one step.\")\n",
    "        return [plan_string.strip()]\n",
    "\n",
    "    return plan_items\n",
    "\n",
    "# --- Helper Function to Extract URLs and Content ---\n",
    "def extract_content_and_urls(text: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Separates synthesized content from 'Source URL:' lines.\"\"\"\n",
    "    urls = []\n",
    "    content_lines = []\n",
    "    source_url_pattern = re.compile(r\"^\\s*Source URL:\\s*(.*)\", re.IGNORECASE)\n",
    "    for line in text.splitlines():\n",
    "        match = source_url_pattern.match(line)\n",
    "        if match:\n",
    "            urls.append(match.group(1).strip())\n",
    "        else:\n",
    "            content_lines.append(line)\n",
    "    content = \"\\n\".join(content_lines).strip()\n",
    "    return content, urls\n",
    "\n",
    "\n",
    "# --- Helper Function to Parse Evaluation and Refinement Tasks ---\n",
    "def parse_evaluation_and_refinement_tasks(eval_output: str) -> Tuple[str, List[str]]:\n",
    "    \"\"\"Parses the evaluator output into evaluation text and refinement tasks.\"\"\"\n",
    "    evaluation_text = \"\"\n",
    "    refinement_tasks = []\n",
    "\n",
    "    # Try to split based on the expected headings\n",
    "    eval_match = re.search(r\"Evaluation:(.*?)Further Research Needed:\", eval_output, re.DOTALL | re.IGNORECASE)\n",
    "    tasks_match = re.search(r\"Further Research Needed:(.*)\", eval_output, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    if eval_match:\n",
    "        evaluation_text = eval_match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback if \"Further Research Needed:\" is missing (e.g., eval only)\n",
    "        eval_only_match = re.search(r\"Evaluation:(.*)\", eval_output, re.DOTALL | re.IGNORECASE)\n",
    "        if eval_only_match:\n",
    "            evaluation_text = eval_only_match.group(1).strip()\n",
    "        else: # If headings not found, assume the whole text is evaluation\n",
    "             evaluation_text = eval_output.strip()\n",
    "\n",
    "\n",
    "    if tasks_match:\n",
    "        tasks_section = tasks_match.group(1).strip()\n",
    "        if tasks_section.lower() != 'none':\n",
    "            # Use the same parsing logic as for the initial plan\n",
    "             refinement_tasks = parse_research_plan(tasks_section) # Reuse plan parsing logic\n",
    "             # Simple line split fallback if parse_research_plan fails\n",
    "             if not refinement_tasks and tasks_section:\n",
    "                  refinement_tasks = [line.strip() for line in tasks_section.splitlines() if line.strip()]\n",
    "\n",
    "    # Ensure evaluation_text is captured even if parsing fails partially\n",
    "    if not evaluation_text and eval_output:\n",
    "         evaluation_text = eval_output # Fallback\n",
    "\n",
    "    return evaluation_text, refinement_tasks\n",
    "\n",
    "\n",
    "# --- Helper Function to Sanitize Filename ---\n",
    "def sanitize_filename(topic: str) -> str:\n",
    "    \"\"\"Removes or replaces characters invalid for filenames.\"\"\"\n",
    "    sanitized = topic.strip().replace(\" \", \"_\")\n",
    "    sanitized = re.sub(r'[\\\\/*?:\"<>|]', \"\", sanitized)\n",
    "    max_len = 100\n",
    "    sanitized = sanitized[:max_len] if len(sanitized) > max_len else sanitized\n",
    "    return f\"{sanitized}.md\" if sanitized else \"research_report.md\"\n",
    "\n",
    "# --- Function to Write Output to Markdown File ---\n",
    "def write_to_markdown(filename: str, topic: str, evaluation_text: str, report_with_refs: str):\n",
    "    \"\"\"Writes the evaluation and the final report (including references) to a markdown file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"# Research Report: {topic}\\n\\n\")\n",
    "            # Write the evaluation text from the evaluator/refiner\n",
    "            f.write(\"## Evaluation of Initial Research\\n\\n\")\n",
    "            f.write(\"```\\n\")\n",
    "            f.write(evaluation_text.strip())\n",
    "            f.write(\"\\n```\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "            # Write the final report generated by the writer\n",
    "            f.write(\"## Final Report Draft\\n\\n\")\n",
    "            f.write(report_with_refs.strip())\n",
    "        print(f\"\\nEvaluation and Report saved to: {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"\\nError writing to file {filename}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during file writing: {e}\")\n",
    "\n",
    "# --- 7. Orchestrate the Pipeline ---\n",
    "def run_research_pipeline(topic: str):\n",
    "    \"\"\"\n",
    "    Runs the research pipeline: Plan -> Research -> Evaluate & Refine -> [Optional Refine Research] -> Write -> Save.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting Research for Topic: {topic} ---\")\n",
    "\n",
    "    initial_evaluation_text = None # Store the text part of the evaluation\n",
    "    final_report = None\n",
    "    all_source_urls = set() # Use a set for unique URLs\n",
    "    aggregated_research_content_list = [] # Store content pieces\n",
    "\n",
    "    # == Step 1: Planning Agent ==\n",
    "    print(\"\\n--- Running Planning Agent ---\")\n",
    "    try:\n",
    "        research_plan_str = planner_chain.invoke({\"topic\": topic})\n",
    "        print(f\"\\n--- Generated Research Plan ---:\\n{research_plan_str}\")\n",
    "        plan_steps = parse_research_plan(research_plan_str)\n",
    "        if not plan_steps:\n",
    "             print(\"Error: Planning agent failed to generate a usable plan.\")\n",
    "             return \"Planning phase failed.\"\n",
    "        print(f\"\\n--- Parsed Plan Steps ({len(plan_steps)}) ---:\")\n",
    "        for i, step in enumerate(plan_steps):\n",
    "            print(f\"{i+1}. {step}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during planning phase: {e}\")\n",
    "        return f\"Planning phase failed: {e}\"\n",
    "\n",
    "    # == Step 2: Initial Researcher Agent Execution ==\n",
    "    print(\"\\n--- Running Initial Research Phase ---\")\n",
    "    research_successful = True\n",
    "    for i, step in enumerate(plan_steps):\n",
    "        print(f\"\\n--- Researching Step {i+1}/{len(plan_steps)}: {step} ---\")\n",
    "        try:\n",
    "            research_result = researcher_agent_executor.invoke({\"research_question\": step})\n",
    "            step_output_raw = research_result['output']\n",
    "            step_content, step_urls = extract_content_and_urls(step_output_raw)\n",
    "\n",
    "            print(f\"\\n--- Synthesized Content for Step {i+1} ---:\\n{step_content}\")\n",
    "            print(f\"--- Sources for Step {i+1} ---:\")\n",
    "            if step_urls:\n",
    "                for url in step_urls: print(f\"- {url}\")\n",
    "                all_source_urls.update(step_urls)\n",
    "            else:\n",
    "                print(\"- No URLs listed.\")\n",
    "\n",
    "            # Store content with header\n",
    "            aggregated_research_content_list.append(f\"--- Initial Research on: {step} ---\\n{step_content}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during research step {i+1} ('{step}'): {e}\")\n",
    "            aggregated_research_content_list.append(f\"--- Initial Research on: {step} ---\\nFAILED: {e}\\n\")\n",
    "            research_successful = False\n",
    "\n",
    "    # Combine initial content for evaluation\n",
    "    initial_aggregated_research_for_eval = \"\\n\".join(aggregated_research_content_list)\n",
    "    if not initial_aggregated_research_for_eval.strip():\n",
    "        print(\"Error: No initial research content was gathered.\")\n",
    "        return \"Initial research phase failed (no content).\"\n",
    "    if not research_successful:\n",
    "         print(\"Warning: Some initial research steps failed.\")\n",
    "\n",
    "    print(\"\\n--- Initial Research Content Aggregated ---\")\n",
    "    print(f\"--- Total Unique Source URLs after initial phase: {len(all_source_urls)} ---\")\n",
    "\n",
    "    # == Step 3: Evaluator & Refiner Agent Execution ==\n",
    "    print(\"\\n--- Running Evaluator & Refiner Agent ---\")\n",
    "    refinement_tasks = []\n",
    "    try:\n",
    "        eval_refine_output = evaluator_refiner_chain.invoke({\n",
    "            \"topic\": topic,\n",
    "            \"research_results\": initial_aggregated_research_for_eval # Pass initial content\n",
    "        })\n",
    "        # Parse the output\n",
    "        initial_evaluation_text, refinement_tasks = parse_evaluation_and_refinement_tasks(eval_refine_output)\n",
    "\n",
    "        print(f\"\\n--- Evaluator Output ---:\\n{initial_evaluation_text}\") # Display evaluation\n",
    "        if refinement_tasks:\n",
    "            print(\"\\n--- Refinement Tasks Identified ---:\")\n",
    "            for i, task in enumerate(refinement_tasks):\n",
    "                print(f\"{i+1}. {task}\")\n",
    "        else:\n",
    "            print(\"\\n--- No Refinement Tasks Identified ---\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation/refinement phase: {e}\")\n",
    "        # Store error in evaluation text, assume no refinement tasks\n",
    "        initial_evaluation_text = f\"Evaluation phase failed: {e}\"\n",
    "        refinement_tasks = []\n",
    "\n",
    "\n",
    "    # == Step 4: Optional Refinement Research Execution ==\n",
    "    if refinement_tasks:\n",
    "        print(\"\\n--- Running Refinement Research Phase ---\")\n",
    "        refinement_successful = True\n",
    "        refined_content_list = []\n",
    "        for i, task in enumerate(refinement_tasks):\n",
    "            print(f\"\\n--- Researching Refinement Task {i+1}/{len(refinement_tasks)}: {task} ---\")\n",
    "            try:\n",
    "                research_result = researcher_agent_executor.invoke({\"research_question\": task})\n",
    "                step_output_raw = research_result['output']\n",
    "                step_content, step_urls = extract_content_and_urls(step_output_raw)\n",
    "\n",
    "                print(f\"\\n--- Synthesized Content for Task {i+1} ---:\\n{step_content}\")\n",
    "                print(f\"--- Sources for Task {i+1} ---:\")\n",
    "                if step_urls:\n",
    "                    for url in step_urls: print(f\"- {url}\")\n",
    "                    all_source_urls.update(step_urls) # Add new URLs\n",
    "                else:\n",
    "                    print(\"- No URLs listed.\")\n",
    "\n",
    "                # Store refined content with header\n",
    "                refined_content_list.append(f\"--- Refined Research on: {task} ---\\n{step_content}\\n\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during refinement research task {i+1} ('{task}'): {e}\")\n",
    "                refined_content_list.append(f\"--- Refined Research on: {task} ---\\nFAILED: {e}\\n\")\n",
    "                refinement_successful = False\n",
    "\n",
    "        if not refinement_successful:\n",
    "            print(\"Warning: Some refinement research steps failed.\")\n",
    "\n",
    "        # Prepend initial results with refined results for the writer\n",
    "        aggregated_research_content_list.extend(refined_content_list) # Add refined content\n",
    "        print(\"\\n--- Refinement Research Content Aggregated ---\")\n",
    "        print(f\"--- Total Unique Source URLs after refinement: {len(all_source_urls)} ---\")\n",
    "\n",
    "    # Combine all research content for the writer\n",
    "    final_combined_research = \"\\n\".join(aggregated_research_content_list)\n",
    "\n",
    "    # == Step 5: Writer Agent Execution ==\n",
    "    if initial_evaluation_text and \"failed:\" not in initial_evaluation_text.lower():\n",
    "        print(\"\\n--- Running Writer Agent ---\")\n",
    "        # Prepare URL list string\n",
    "        url_list_str = \"\\n\".join(f\"- {url}\" for url in sorted(list(all_source_urls)))\n",
    "        if not url_list_str: url_list_str = \"No source URLs were collected.\"\n",
    "\n",
    "        try:\n",
    "            final_report = writer_chain.invoke({\n",
    "                \"topic\": topic,\n",
    "                \"evaluation_text\": initial_evaluation_text, # Pass the evaluator's assessment\n",
    "                \"combined_research_findings\": final_combined_research, # Pass all content\n",
    "                \"source_urls\": url_list_str\n",
    "            })\n",
    "            print(f\"\\n--- Writer Output (Final Report Draft) ---:\\n{final_report}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during writing phase: {e}\")\n",
    "            final_report = f\"Writing phase failed: {e}\"\n",
    "    else:\n",
    "         final_report = \"Skipped due to evaluation failure.\"\n",
    "         print(\"\\n--- Skipping Writer Agent due to Evaluation Failure ---\")\n",
    "\n",
    "    # == Step 6: Write to Markdown File ==\n",
    "    # Check if we have valid evaluation text and a successful final report\n",
    "    if initial_evaluation_text and final_report and \\\n",
    "       \"failed:\" not in initial_evaluation_text.lower() and \\\n",
    "       \"failed:\" not in final_report.lower() and \\\n",
    "       \"skipped\" not in final_report.lower():\n",
    "        md_filename = sanitize_filename(topic)\n",
    "        # Pass the initial evaluation text and the final report from the writer\n",
    "        write_to_markdown(md_filename, topic, initial_evaluation_text, final_report)\n",
    "    else:\n",
    "        print(\"\\n--- Skipping file writing due to errors or skipped steps ---\")\n",
    "\n",
    "    print(\"\\n--- Research Pipeline Complete ---\")\n",
    "    # Return the final report content or the last relevant error message\n",
    "    if final_report and \"failed:\" not in final_report.lower() and \"skipped\" not in final_report.lower():\n",
    "        return final_report\n",
    "    elif initial_evaluation_text and \"failed:\" in initial_evaluation_text.lower():\n",
    "        return initial_evaluation_text\n",
    "    else:\n",
    "        return \"Pipeline encountered an error or was skipped.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_research_pipeline(\"Compare the recent marketing strategy between Coca Cola and Pepsi\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}